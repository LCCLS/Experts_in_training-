{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4b MA : Build your own corpus exploration tool\n",
    "\n",
    "**Deadline for Assignment 4a+b: Friday October 15, 2021, before 14:30 via Canvas (Assignment 4)** \n",
    "\n",
    "\n",
    "In this assignment, you're going to build your own tool for exploring a the **Parallel Meaning Bank** (PMB). This resource is a **parallel corpus**, which means that it contains the **same documents translated into multiple languages**. Such resources are very valuable for many aspects of linguistics and Natural Language Processing (NLP), but most importantly for Machine Translation (ML). \n",
    "\n",
    "For this part of assignment 4, you will submit two python scripts called:\n",
    "\n",
    "* `explore_pmb.py`\n",
    "* `utils.py`\n",
    "\n",
    "The corpus contains a lot of data, but not every document is translated into every language. Therefore, we will build a tool which explores different aspects of coverage. Your tool will be able to:\n",
    "\n",
    "* explore the **overall coverage per language**\n",
    "* explore the the **parallel coverage of a given language pair** (i.e. how many documents and tokens exist in a language pair?)\n",
    "* **browse parallel text** in given language pairs\n",
    "\n",
    "Before diving into building the tool, we're going to guide you through a couple of warm-up examples. You can use them to explore the data structure and write your code. It is permitted to copy-paste bits of code (you will have to modify them to solve all exercises). \n",
    "\n",
    "The assignment is structured as follows:\n",
    "\n",
    "1. Understanding the data structure (code snippets to guide you through the corpus)\n",
    "2. Writing functions (writing the actual code)\n",
    "3. Putting the tool together (combining the code)\n",
    "4. Testing and submission (a final check of whether your code does what it is supposed to do)\n",
    "\n",
    "\n",
    "You can learn more about the PMB [here](https://pmb.let.rug.nl/). \n",
    "\n",
    "If you have **questions** about this chapter, please contact us at cltl.python.course@gmail.com. Questions and answers will be collected in [this Q&A document](https://docs.google.com/document/d/1551Db87zckRPbKDosZ4105htEUxVWZu9ejDj3MM8qck/edit?usp=sharing), so please check it before you email. \n",
    "\n",
    "**Tip**: Read the entire assignment before you start writing code. Try to understand the tool we're building before you start. Making notes with pen and paper can be very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the data structure\n",
    "\n",
    "In this part, we guide you through the data structure. You can use the code below for the rest of your assignment. You can play with the code and add things to it, but you will not receive points in this part. Its purpose is to make you familiar with the data structure.  \n",
    "\n",
    "### 1.a Download the data\n",
    "\n",
    "1.) Please go to this website: [here](https://pmb.let.rug.nl/data.php)\n",
    "\n",
    "2.) Select version 2.1.0 (the latest version is too big for our purposes) and store the zip file as `PMB/pmb-2.1.0.zip` on your computer (remember where).\n",
    "\n",
    "3.) Unpack the data. You can do this from the terminal by navigating to the directory using `cd`. You should be able to run `unzip pmb-2.1.0.zip` to unzip the file. Alternatively, you can simply unzip by clicking on it. Attention: Unpacking the file may take a while. \n",
    "\n",
    "Use the cell below to assign the path to the data to a variable. We will only consider the gold data for this assignment, therefore you can add the gold directory to the path.\n",
    "\n",
    "Path: `'PMB/pmb-2.1.0/data/gold/'`\n",
    "\n",
    "**Please run the following cell to check if your data are in the right place. If they are, it will not print anything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#my_path = #insert path to the directory containing the PMB on your computer\n",
    "# e.g.:\n",
    "# my_path = '/Users/piasommerauer/Data/'\n",
    "\n",
    "path_pmb = f'{my_path}PMB/pmb-2.1.0/data/gold'\n",
    "assert os.path.isdir(path_pmb), 'corpus data not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Parallel documents \n",
    "Before we can build anything, we have to understand how the data are strucutred. We start by looking at a single document. \n",
    "\n",
    "Parallel documents are stored in the same document directory (d+number). The filenames indicate the language (e.g. en = English). The data we're interested in are stored in .xml format. Run the cell below to inspect the filepaths of a single document. Feel free to modify the path to inspect other documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/'\n",
    "test_doc_files = glob.glob(f'{test_doc_path}*.xml')\n",
    "\n",
    "for f in test_doc_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c XML structure of a single document\n",
    "\n",
    "Below, we access a single document and load the xml structure using lxml.etree. Run the cell to print the xml tree. \n",
    "\n",
    "Explore the document structure and try to answer these questions:\n",
    "\n",
    "* Where can you find the full text of the document?\n",
    "* Where can you find information about each token in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "test_doc_path_en = test_doc_path+'en.drs.xml'\n",
    "doc_tree = etree.parse(test_doc_path_en)\n",
    "doc_root = doc_tree.getroot()\n",
    "etree.dump(doc_root, pretty_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing functions\n",
    "\n",
    "In this part of the assigment, we guide you through writing the functions for your tool. Feel free to use the notebook for exploration, but your final functions should be stored in `utils.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Get all token elements of a document in a given language\n",
    "\n",
    "Write a function which fulfills the following requirements: \n",
    "\n",
    "* Positional parameter: path to the document in a particular language \n",
    "* Output: list of token elements (the token elements are called 'tagtoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(path_to_doc):\n",
    "    \n",
    "    # your code here \n",
    "    pass \n",
    "    \n",
    "# Test you function\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "language = 'en'\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/{language}.drs.xml'\n",
    "# Function call\n",
    "tokens = get_tokens(test_doc_path)\n",
    "\n",
    "assert len(tokens) == 6 and type(tokens[1]) == etree._Element, 'token list not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Get token and pos from a token element\n",
    "\n",
    "Write a function which fulfills the following requirements: \n",
    "\n",
    "* Positional parameter: token element\n",
    "* Output: token (string) and part of speech tag (string) of the token element\n",
    "\n",
    "An example token element is shown below. (You can use it for testing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_str = \"\"\"\n",
    " <tagtoken xml:id=\"i1002\">\n",
    "   <tags>\n",
    "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
    "     <tag type=\"tok\">'m</tag>\n",
    "     <tag type=\"sym\">be</tag>\n",
    "     <tag type=\"lemma\">be</tag>\n",
    "     <tag type=\"from\">1</tag>\n",
    "     <tag type=\"to\">3</tag>\n",
    "     <tag type=\"pos\">VBP</tag>\n",
    "     <tag type=\"sem\">NOW</tag>\n",
    "     <tag type=\"wordnet\">O</tag>\n",
    "   </tags>\n",
    " </tagtoken>\n",
    "\"\"\"\n",
    "\n",
    "test_token = etree.fromstring(test_token_str)\n",
    "print(test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_pos(token_element):\n",
    "    pass\n",
    "\n",
    "# Test your function using the first token \n",
    "token, pos = get_token_pos(test_token)\n",
    "assert token == \"'m\" and pos == 'VBP', 'token and pos not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Get document text\n",
    "\n",
    "Define a function with the following requirements:\n",
    "\n",
    "* Positional parameter: filepath of a document in a particular language (i.e. full, relativ filepath)\n",
    "* Output: the text of the document as a string\n",
    "\n",
    "**Hint**:\n",
    " \n",
    "There are two options to get the document text of a file:\n",
    "\n",
    "* Option 1: Access the comment indicated by `<!--  -->`. Look at the file above to find the comment. You will see that it contains the entire text represented in the xml file. You can access it by iterating over the child-elements of the root. Try this out in the notebook before defining your function. You can get started using the code below.\n",
    "\n",
    "\n",
    "* Option 2: Use the tokens. You can collect all the tokens in a document using the functions we have defined above. Once you have all tokens, you can use a string method to join them with whitespaces between them.\n",
    "\n",
    "Only implement **one** of these options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippet for option 1 \n",
    "\n",
    "# use the test document\n",
    "test_doc_path_en = test_doc_path\n",
    "# load\n",
    "doc_tree = etree.parse(test_doc_path_en)\n",
    "# get root \n",
    "root = doc_tree.getroot()\n",
    "# iterate over child-elements\n",
    "for ch in root.getchildren():\n",
    "    print('tag', ch.tag)\n",
    "    print('text', ch.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_text(path_to_doc):\n",
    "    #Your code here\n",
    "    pass\n",
    "\n",
    "# Test you function\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "language = 'en'\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/{language}.drs.xml'\n",
    "\n",
    "text = get_doc_text(test_doc_path)\n",
    "\n",
    "assert text == \"I 'm not tired at~all .\", 'doc text not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d Sort documents on languages \n",
    "\n",
    "To explore the coverage of the corpus, it is convenient to store the documents as sets mapped to their language. We can then use set methods (i.e. intersection) to check which documents exist in a given language pair. \n",
    "\n",
    "Write a function which fulfills the following criteria:\n",
    "\n",
    "* mandatory positional argument: path to the corpus (e.g. '../../../Data/PMB/pmb-2.1.0/data/gold')\n",
    "* output: a dictionary of the following format:\n",
    "            `{\n",
    "              'language1': {'path_to_doc1', 'path_to_doc2', ...},\n",
    "              'language2': {'path_to_doc1', 'path_to_doc4', ...},\n",
    "              'language3': {'path_to_doc2', 'path_to_doc3', ...},\n",
    "              }`\n",
    "       \n",
    "       \n",
    "Hints:\n",
    "\n",
    "* filepaths are strings; you can manipulate them using string methods (e.g. split on '/' or '.'). \n",
    "* The os mudule has a convenient way of extracting the filename from a long path (i.e. the last bit of the path): os.path.basename(your_path)\n",
    "* Feel free to use [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) (with a set as the default value) (`from collections import defaultdict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for filepath manipulation:\n",
    "import os \n",
    "\n",
    "my_path = '../../some/dir/containing/a/file/with/an/interesting/name.txt'\n",
    "f_name = os.path.basename(my_path)\n",
    "print(f_name)\n",
    "remaining_path = my_path.rstrip(f_name)\n",
    "print(remaining_path)\n",
    "name, extension = f_name.split('.')\n",
    "print(name, extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_documents(path_pmb):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test you function:\n",
    "language_doc_dict = sort_documents(path_pmb)\n",
    "\n",
    "n_en = len(language_doc_dict['en'])\n",
    "n_it = len(language_doc_dict['it'])\n",
    "n_de = len(language_doc_dict['de'])\n",
    "n_nl = len(language_doc_dict['nl'])\n",
    "\n",
    "assert n_en == 4555 and n_it == 635 and n_de == 1175 and n_nl == 586, 'sorting not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Putting the tool together\n",
    "\n",
    "Congratulations! You've written most of the code already! \n",
    "\n",
    "From now on, we will mostly use the functions defined above and combine them in the file called `explore_pmb.py`. \n",
    "\n",
    "Some code snippets are provided here to help you along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Printing statistics for all languages\n",
    "\n",
    "Let's start by exploring the coverage of all languages individually. In `explore_pmb.py`, write the following code:\n",
    "\n",
    "* Import the function`sort_docs`, call it and assign the output dictionary to a variable called `language_doc_dict`. Don't forget to define the path to the corpus, which you use as function input. \n",
    "* Create a list of all languages (hint: you can simply use the keys of `language_doc_dict`). \n",
    "* For each language, print the following:\n",
    "    `[Language]: num docs: [number of documents], num tokens: [number of tokens]\n",
    "    \n",
    "Hints:\n",
    "\n",
    "* Each document is unique - you can simply count the elements in the sets to get the number of documents.\n",
    "* Use the function `get_tokens` to access the tokens. Then count them.\n",
    "* You will most likly use two nested loops: An outer loop for languages and an inner loop to access the tokens in the documents. \n",
    "* Use f-strings to print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code to help you along the way (you can also start from scratch)\n",
    "languages = # your code here\n",
    "\n",
    "for languagage in languages:\n",
    "    n_docs = # your code here\n",
    "    n_tokens = # your code here\n",
    "    docs = # your code here\n",
    "    # your code here\n",
    "    for doc in docs:\n",
    "        path_to_doc = f'{doc}/{language}.drs.xml'\n",
    "        tokens = get_tokens(path_to_doc)\n",
    "        # your code here\n",
    "    print(f'{language}: num docs: {n_docs}, num tokens: {n_tokens}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Printing statistics for language pairs \n",
    "\n",
    "We also want to explore the coverage of **parallel data** for the language pairs in the corpus. To do this, use an additional loop to iterate over all possible language pairs in the parallal meaning bank and print the number of documents which exist for both languages. \n",
    "\n",
    "Use the function below to generate the language pairs. Simply copy-paste it into the script called `utils.py` and import it into `explore_pmb.py`. Use the cell below to explore how it works. \n",
    "\n",
    "The list of language pair should look similar to this (and contain all possible pairs):\n",
    "\n",
    "`pairs = [(‘nl’, ‘en’), (‘it’, ‘de’), (‘en’, ‘it’)]`\n",
    "\n",
    "Print the following for each language pair (use f-strings):\n",
    "\n",
    "`Coverage for parallel data in [language_1] and [language_2]: [number of documents]`\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You can unpack tuples in a loop.\n",
    "* Use a set method to get the document paths covered by both languages. Then simply count the paths.\n",
    "* You do not have to match the file-contents. Instead, use the information provided in the filepaths (in a previous step, you have sorted your corpus files according to language).  The file paths in the sets (representing the documents) are supposed to consist of the base names only (i.e. no directory paths). You can use set operations to get the overlap between two languages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(language_list):\n",
    "    \"\"\"Given a list, return a list of tuples of all element pairs.\"\"\"\n",
    "    pairs = []\n",
    "    for l1 in language_list:\n",
    "        for l2 in language_list:\n",
    "            if l1 != l2:\n",
    "                if (l1, l2) not in pairs and (l2, l1) not in pairs:\n",
    "                    pairs.append((l1, l2))\n",
    "    return pairs\n",
    "\n",
    "language_list = ['a', 'b', 'c']\n",
    "pairs = get_pairs(language_list)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a start (feel free to modify this)\n",
    "\n",
    "for lang1, lang2 in pairs:\n",
    "    docs_lang1 = language_doc_dict[lang1]\n",
    "    docs_lang2 = language_doc_dict[lang2]\n",
    "    # you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Explore parallel documents \n",
    "\n",
    "As a final step, we want let the user browse the parallel documents in a chosen language pair. Write the following code (in `explore_pmb.py`):\n",
    "\n",
    "* use input() to define two variables: language_1 and language_2\n",
    "* get the document paths for all documents covered by both languages\n",
    "* Loop over the documents and print the documents in both languages. After each document, ask the user whether they want to continue. If the answer is 'no', stop. Else, show the next. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Come up with your own comparison\n",
    "\n",
    "Got insterested in parallel data? Extract a comparison you find interesting! \n",
    "\n",
    "**This is an additional exercise - it is not required to complete this to get a full score.** \n",
    "\n",
    "If you complete this exercise, you can earn up to 3 additional points which can be used to make up for other points you missed. Note that you cannot get more than a full score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing & submission\n",
    "\n",
    "Congratulations! You've built a corpus exploration tool! Before you submit, please make sure to test your code:\n",
    "\n",
    "* Can you run the script `explore_pmb.py` from the command line?\n",
    "* Do you get a general corpus overview (see 3.a)?\n",
    "* Do you get an overview of language pairs (see 3.b)?\n",
    "* Are you asked to provide a language pair and do you see examples of parallel documents once you entered a pair (see 3.c?)\n",
    "\n",
    "If you did not manage to complete all of the exercises, submit what you have and, if possible, explain how you were going to solve them. You get points for intermediate steps. \n",
    "\n",
    "**Please submit python scripts only. You can use this notebook for exploration and development, but we will not consider the code written here.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (teaching-3.8.3)",
   "language": "python",
   "name": "teaching-3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
